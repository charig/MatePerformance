<<TracingPerformance, echo=FALSE, message=FALSE>>=
tracing <- getWarmedupData(
  paste(root_path, "/../Data/tracing.data", sep=""), 
  c("Value", "Benchmark", "VM", "Suite", "Iteration"), 
  vmNamesMap(), 
  c("MATEpe", "MATEmt"),
  numberOfIterationsPerBenchmark)

tracing <- ddply(tracing, ~ Benchmark + VM , transform, 
	Var = grepl("Trace", Benchmark),
	Benchmark = gsub("Trace|Big|Instrumented", "", Benchmark))

levels(tracing$VM) <- c(levels(tracing$VM), "MATEpe-baseline", "MATEmt-baseline") 
tracing$VM[tracing$Var == FALSE & tracing$VM == "MATEpe"] <- "MATEpe-baseline"
tracing$VM[tracing$Var == FALSE & tracing$VM == "MATEmt"] <- "MATEmt-baseline"

tracingMT <- droplevels(subset(tracing, VM %in% mtVMs | VM == "MATEmt-baseline"))
tracingPE <- droplevels(subset(tracing, VM %in% peVMs | VM == "MATEpe-baseline"))

baselineMT <- droplevels(subset(tracing, VM == "MATEmt-baseline"))
baselinePE <- droplevels(subset(tracing, VM == "MATEpe-baseline"))
@
\def\TracingPerfComparisonPlot{%
<<perf-overview-tracing, fig.cap='Overhead factors for profiling the amount of method activations in a set of benchmarks.', fig.align='center', out.width = "0.8\\textwidth", fig.width = 7, fig.height=3>>=
boxplotMateToSom(list(tracingMT, tracingPE), list('MATEmt-baseline', 'MATEpe-baseline'), "Overhead Factor", "Benchmark", "Readonly Approaches Overhead")
@
}%

\def\TracingTableSummary{%
<<perf-summary-table-tracing>>=
tracingSummaryPE <- summarizedPerBenchmark(tracingPE, baselinePE, "MATEpe-baseline", c(0,10000))
tracingSummaryMT <- summarizedPerBenchmark(tracingMT, baselineMT, "MATEmt-baseline", c(0,10000))
tracingSummary <- rbind(tracingSummaryPE, tracingSummaryMT)[,c(2,1,3,4:8)]
print(
  kable(arrange(tracingSummary, Benchmark), 
       booktabs = T,
       format = "latex",
       longtable = T,
       caption = "Overall Results for the Profiling Benchmarks",
       digits = 2)  %>%
       kable_styling(latex_options = c("repeat_header"), font_size = 7)  %>%
       collapse_rows(columns = 1:2))
@
}